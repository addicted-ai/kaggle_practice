{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "titanic_1d_cnn.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/addicted-ai/kaggle_practice/blob/main/titanic_1d-cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9XERJaP0x_R"
      },
      "source": [
        "# supressing warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "id": "H9XERJaP0x_R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import os\n",
        "import copy\n",
        "from copy import deepcopy as dp\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import StandardScaler\n"
      ],
      "metadata": {
        "id": "6VLpHw9H9Su8"
      },
      "id": "6VLpHw9H9Su8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "218ebb8d"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "id": "218ebb8d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "whwl_XCv9RD8"
      },
      "id": "whwl_XCv9RD8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wkwHs3lLLXu"
      },
      "source": [
        "# Reading the dataset\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/addicted-ai/kaggle_practice/main/dataset/titanic/train.csv')\n",
        "test = pd.read_csv('https://raw.githubusercontent.com/addicted-ai/kaggle_practice/main/dataset/titanic/test.csv')"
      ],
      "id": "_wkwHs3lLLXu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "Er6D8jCKLNeA",
        "outputId": "3be2712a-5d92-4e23-9f71-e2e4f66d0083"
      },
      "source": [
        "# data to be used for training & with labeled dependent variable\n",
        "df.head(4)"
      ],
      "id": "Er6D8jCKLNeA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Survived</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>Braund, Mr. Owen Harris</td>\n",
              "      <td>male</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>A/5 21171</td>\n",
              "      <td>7.2500</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Cumings, Mrs. John Bradley (Florence Briggs Thayer)</td>\n",
              "      <td>female</td>\n",
              "      <td>38.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>PC 17599</td>\n",
              "      <td>71.2833</td>\n",
              "      <td>C85</td>\n",
              "      <td>C</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Heikkinen, Miss. Laina</td>\n",
              "      <td>female</td>\n",
              "      <td>26.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>STON/O2. 3101282</td>\n",
              "      <td>7.9250</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
              "      <td>female</td>\n",
              "      <td>35.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>113803</td>\n",
              "      <td>53.1000</td>\n",
              "      <td>C123</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Survived  Pclass  \\\n",
              "0            1         0       3   \n",
              "1            2         1       1   \n",
              "2            3         1       3   \n",
              "3            4         1       1   \n",
              "\n",
              "                                                  Name     Sex   Age  SibSp  \\\n",
              "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
              "1  Cumings, Mrs. John Bradley (Florence Briggs Thayer)  female  38.0      1   \n",
              "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
              "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
              "\n",
              "   Parch            Ticket     Fare Cabin Embarked  \n",
              "0      0         A/5 21171   7.2500   NaN        S  \n",
              "1      0          PC 17599  71.2833   C85        C  \n",
              "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
              "3      0            113803  53.1000  C123        S  "
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "mJ4Yc5S8LOpT",
        "outputId": "53f731d2-2572-4e3a-c965-f6f370446716"
      },
      "source": [
        "# test data that i have to get prediction & submit\n",
        "test.head(4)"
      ],
      "id": "mJ4Yc5S8LOpT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PassengerId</th>\n",
              "      <th>Pclass</th>\n",
              "      <th>Name</th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>SibSp</th>\n",
              "      <th>Parch</th>\n",
              "      <th>Ticket</th>\n",
              "      <th>Fare</th>\n",
              "      <th>Cabin</th>\n",
              "      <th>Embarked</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>892</td>\n",
              "      <td>3</td>\n",
              "      <td>Kelly, Mr. James</td>\n",
              "      <td>male</td>\n",
              "      <td>34.5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>330911</td>\n",
              "      <td>7.8292</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>893</td>\n",
              "      <td>3</td>\n",
              "      <td>Wilkes, Mrs. James (Ellen Needs)</td>\n",
              "      <td>female</td>\n",
              "      <td>47.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>363272</td>\n",
              "      <td>7.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>894</td>\n",
              "      <td>2</td>\n",
              "      <td>Myles, Mr. Thomas Francis</td>\n",
              "      <td>male</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>240276</td>\n",
              "      <td>9.6875</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Q</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>895</td>\n",
              "      <td>3</td>\n",
              "      <td>Wirz, Mr. Albert</td>\n",
              "      <td>male</td>\n",
              "      <td>27.0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>315154</td>\n",
              "      <td>8.6625</td>\n",
              "      <td>NaN</td>\n",
              "      <td>S</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   PassengerId  Pclass                              Name     Sex   Age  SibSp  \\\n",
              "0          892       3                  Kelly, Mr. James    male  34.5      0   \n",
              "1          893       3  Wilkes, Mrs. James (Ellen Needs)  female  47.0      1   \n",
              "2          894       2         Myles, Mr. Thomas Francis    male  62.0      0   \n",
              "3          895       3                  Wirz, Mr. Albert    male  27.0      0   \n",
              "\n",
              "   Parch  Ticket    Fare Cabin Embarked  \n",
              "0      0  330911  7.8292   NaN        Q  \n",
              "1      0  363272  7.0000   NaN        S  \n",
              "2      0  240276  9.6875   NaN        Q  \n",
              "3      0  315154  8.6625   NaN        S  "
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9cjNwcSLP4n",
        "outputId": "853ee300-157f-448d-8075-c8b611dace63"
      },
      "source": [
        "df.info()"
      ],
      "id": "w9cjNwcSLP4n",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891 entries, 0 to 890\n",
            "Data columns (total 12 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   PassengerId  891 non-null    int64  \n",
            " 1   Survived     891 non-null    int64  \n",
            " 2   Pclass       891 non-null    int64  \n",
            " 3   Name         891 non-null    object \n",
            " 4   Sex          891 non-null    object \n",
            " 5   Age          714 non-null    float64\n",
            " 6   SibSp        891 non-null    int64  \n",
            " 7   Parch        891 non-null    int64  \n",
            " 8   Ticket       891 non-null    object \n",
            " 9   Fare         891 non-null    float64\n",
            " 10  Cabin        204 non-null    object \n",
            " 11  Embarked     889 non-null    object \n",
            "dtypes: float64(2), int64(5), object(5)\n",
            "memory usage: 83.7+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfcydDBEUFo1"
      },
      "source": [
        "- Categorical columns are `Name`, `Sex`, `Ticket`, `Cabin`, `Embarked`.\n",
        "- Name column can't be used for model."
      ],
      "id": "GfcydDBEUFo1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEXjWoCNi5Yu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8480526b-e696-4ef2-b5cd-30ee31a6a8fe"
      },
      "source": [
        "test.info()"
      ],
      "id": "hEXjWoCNi5Yu",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 418 entries, 0 to 417\n",
            "Data columns (total 11 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   PassengerId  418 non-null    int64  \n",
            " 1   Pclass       418 non-null    int64  \n",
            " 2   Name         418 non-null    object \n",
            " 3   Sex          418 non-null    object \n",
            " 4   Age          332 non-null    float64\n",
            " 5   SibSp        418 non-null    int64  \n",
            " 6   Parch        418 non-null    int64  \n",
            " 7   Ticket       418 non-null    object \n",
            " 8   Fare         417 non-null    float64\n",
            " 9   Cabin        91 non-null     object \n",
            " 10  Embarked     418 non-null    object \n",
            "dtypes: float64(2), int64(4), object(5)\n",
            "memory usage: 36.0+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9mHo-ClHUs1k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "outputId": "45cb8d44-30b7-4d0e-e059-02ade76e3514"
      },
      "source": [
        "cat_col = ['Sex', 'Ticket', 'Cabin', 'Embarked']\n",
        "for i in cat_col:\n",
        "  print(i,':')\n",
        "  display(df[i].value_counts(dropna=False))\n",
        "  print('________\\n')"
      ],
      "id": "9mHo-ClHUs1k",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sex :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "male      577\n",
              "female    314\n",
              "Name: Sex, dtype: int64"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "________\n",
            "\n",
            "Ticket :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "CA. 2343             7\n",
              "347082               7\n",
              "1601                 7\n",
              "347088               6\n",
              "3101295              6\n",
              "                    ..\n",
              "STON/O 2. 3101289    1\n",
              "2620                 1\n",
              "343275               1\n",
              "315096               1\n",
              "233639               1\n",
              "Name: Ticket, Length: 681, dtype: int64"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "________\n",
            "\n",
            "Cabin :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "NaN            687\n",
              "G6               4\n",
              "C23 C25 C27      4\n",
              "B96 B98          4\n",
              "F2               3\n",
              "              ... \n",
              "B94              1\n",
              "B86              1\n",
              "C82              1\n",
              "C106             1\n",
              "F G63            1\n",
              "Name: Cabin, Length: 148, dtype: int64"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "________\n",
            "\n",
            "Embarked :\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "S      644\n",
              "C      168\n",
              "Q       77\n",
              "NaN      2\n",
              "Name: Embarked, dtype: int64"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "________\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-MQ-hwcTYeE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "54934409-1d81-458f-e372-7413dd100b52"
      },
      "source": [
        "print('No of Unique values in Cabin:')\n",
        "display(df['Cabin'].nunique())\n",
        "print('No of Unique values in Ticket:')\n",
        "display(df['Ticket'].nunique())"
      ],
      "id": "H-MQ-hwcTYeE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of Unique values in Cabin:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "147"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "No of Unique values in Ticket:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "681"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vn4qbM7V6Dj"
      },
      "source": [
        "- Both Ticket & Cabin have very high no of level. We can't use them for training.\n",
        "- 'Age' Columns seems to have ~20% NaN values. We can drop it.\n",
        "- 'Embarked` column has 2 NaN values. We can impute NaN with mode of column."
      ],
      "id": "3vn4qbM7V6Dj"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXr07MYZX6bz"
      },
      "source": [
        "df['Embarked'] = df['Embarked'].replace(np.nan, df['Embarked'].mode()[0])"
      ],
      "id": "PXr07MYZX6bz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Y2hfnL1jNIU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adc74b87-e724-438f-dc3c-071577883c49"
      },
      "source": [
        "df['Fare'].describe(percentiles=[0.1, 0.25, 0.5, 0.75, .8, 0.9, 0.95, 0.97, 0.99, 1])"
      ],
      "id": "2Y2hfnL1jNIU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "count    891.000000\n",
              "mean      32.204208\n",
              "std       49.693429\n",
              "min        0.000000\n",
              "10%        7.550000\n",
              "25%        7.910400\n",
              "50%       14.454200\n",
              "75%       31.000000\n",
              "80%       39.687500\n",
              "90%       77.958300\n",
              "95%      112.079150\n",
              "97%      151.550000\n",
              "99%      249.006220\n",
              "100%     512.329200\n",
              "max      512.329200\n",
              "Name: Fare, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04mFh8NnjTQ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd932658-f123-48ba-c00d-06d875246580"
      },
      "source": [
        "df['Fare'].median()"
      ],
      "id": "04mFh8NnjTQ1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14.4542"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LaHMaFVjEbF"
      },
      "source": [
        "test['Fare'] = test['Fare'].replace(np.nan, df['Fare'].median())"
      ],
      "id": "_LaHMaFVjEbF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMJuj23lWvyA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ecc5f1b-b801-4d7a-bec7-80c32dcf38d0"
      },
      "source": [
        "df.columns"
      ],
      "id": "IMJuj23lWvyA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n",
              "       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mei_qlEMQpRU"
      },
      "source": [
        "features = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked', 'Survived']\n",
        "df = df[features]\n",
        "test = test[['Pclass', 'Sex', 'SibSp', 'Parch', 'Fare', 'Embarked']]"
      ],
      "id": "mei_qlEMQpRU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUniAgCNzx1d",
        "outputId": "564e84d4-ec5f-4dbb-df59-3c76048d172b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "splits = RandomSplitter(valid_pct=0.2, seed=0.42)(range_of(df))\n",
        "splits[0],splits[1]"
      ],
      "id": "RUniAgCNzx1d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((#713) [673,88,154,99,112,717,56,29,159,433...],\n",
              " (#178) [890,70,352,819,208,78,373,578,416,630...])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def scaler_fit_transform(df_1, save_sc = True, sc_name = 'zsco'):   \n",
        "    from sklearn.preprocessing import StandardScaler,MinMaxScaler,MaxAbsScaler,RobustScaler,Normalizer,QuantileTransformer,PowerTransformer\n",
        "    '''\n",
        "    Inputs: df_1, save_ft, sc_name\n",
        "\n",
        "    df_1    : Pandas DataFrame to Scale\n",
        "    save_sc : bool\n",
        "              Return Scaler Object\n",
        "    sc_name : str, Scaler name\n",
        "              Available Scalers\n",
        "                zsco  : StandardScaler\n",
        "                minma : MinmaxScaler\n",
        "                maxb  : MaxAbsScaler\n",
        "                robu  :\n",
        "                norm  :\n",
        "                quan  :\n",
        "                powe  :\n",
        "    '''\n",
        "    ss_1_dic = {'zsco':StandardScaler(),\n",
        "                'mima':MinMaxScaler(),\n",
        "                'maxb':MaxAbsScaler(), \n",
        "                'robu':RobustScaler(),\n",
        "                'norm':Normalizer(), \n",
        "                'quan':QuantileTransformer(n_quantiles=100,random_state=0, output_distribution=\"normal\"),\n",
        "                'powe':PowerTransformer()}\n",
        "    ss_1 = ss_1_dic[sc_name]\n",
        "    df_2 = pd.DataFrame(ss_1.fit_transform(df_1),index = df_1.index,columns = df_1.columns)\n",
        "    if save_ft == False:\n",
        "        return(df_2)\n",
        "    else:\n",
        "        return(df_2, ss_1)"
      ],
      "metadata": {
        "id": "HvaPxXLv9hte"
      },
      "id": "HvaPxXLv9hte",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scaler_transform(df_1, sc_x):\n",
        "    df_2 = pd.DataFrame(sc_x.transform(df_1),index = df_1.index,columns = df_1.columns)\n",
        "    return(df_2)"
      ],
      "metadata": {
        "id": "DMZ5h4O9_TlD"
      },
      "id": "DMZ5h4O9_TlD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed=42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "  \n",
        "seed_everything()"
      ],
      "metadata": {
        "id": "8nE9obm0_6dh"
      },
      "id": "8nE9obm0_6dh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HyperParameters\n",
        "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "EPOCHS = 25\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 1e-3\n",
        "WEIGHT_DECAY = 1e-5\n",
        "NFOLDS = 5\n",
        "EARLY_STOPPING_STEPS = 10\n",
        "EARLY_STOP = False\n",
        "\n",
        "n_comp1 = 50\n",
        "n_comp2 = 15\n",
        "\n",
        "num_features=len(feature_cols) + n_comp1 + n_comp2\n",
        "num_targets=len(target_cols)\n",
        "num_targets_0=len(target_nonsc_cols2)\n",
        "hidden_size=4096\n",
        "\n",
        "tar_freq = np.array([np.min(list(g_table(train[target_cols].iloc[:,i]).values())) for i in range(len(target_cols))])\n",
        "tar_weight0 = np.array([np.log(i+100) for i in tar_freq])\n",
        "tar_weight0_min = dp(np.min(tar_weight0))\n",
        "tar_weight = tar_weight0_min/tar_weight0\n",
        "pos_weight = torch.tensor(tar_weight).to(DEVICE)\n",
        "from torch.nn.modules.loss import _WeightedLoss"
      ],
      "metadata": {
        "id": "U7BJcSkjAZm1"
      },
      "id": "U7BJcSkjAZm1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SmoothBCEwLogits(_WeightedLoss):\n",
        "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
        "        super().__init__(weight=weight, reduction=reduction)\n",
        "        self.smoothing = smoothing\n",
        "        self.weight = weight\n",
        "        self.reduction = reduction\n",
        "\n",
        "    @staticmethod\n",
        "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
        "        assert 0 <= smoothing < 1\n",
        "        with torch.no_grad():\n",
        "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
        "        return targets\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
        "            self.smoothing)\n",
        "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight,\n",
        "                                                  pos_weight = pos_weight)\n",
        "\n",
        "        if  self.reduction == 'sum':\n",
        "            loss = loss.sum()\n",
        "        elif  self.reduction == 'mean':\n",
        "            loss = loss.mean()\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "vzhK-E6jAhrM"
      },
      "id": "vzhK-E6jAhrM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainDataset:\n",
        "    def __init__(self, features, targets):\n",
        "        self.features = features\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.features.shape[0])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        dct = {\n",
        "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
        "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
        "        }\n",
        "        return dct\n",
        "\n",
        "class TestDataset:\n",
        "    def __init__(self, features):\n",
        "        self.features = features\n",
        "\n",
        "    def __len__(self):\n",
        "        return (self.features.shape[0])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        dct = {\n",
        "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
        "        }\n",
        "        return dct"
      ],
      "metadata": {
        "id": "DHGQlT_PAmD1"
      },
      "id": "DHGQlT_PAmD1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
        "    model.train()\n",
        "    final_loss = 0\n",
        "\n",
        "    for data in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        final_loss += loss.item()\n",
        "\n",
        "    final_loss /= len(dataloader)\n",
        "\n",
        "    return final_loss\n",
        "\n",
        "\n",
        "def valid_fn(model, loss_fn, dataloader, device):\n",
        "    model.eval()\n",
        "    final_loss = 0\n",
        "    valid_preds = []\n",
        "\n",
        "    for data in dataloader:\n",
        "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "\n",
        "        final_loss += loss.item()\n",
        "        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
        "\n",
        "    final_loss /= len(dataloader)\n",
        "    valid_preds = np.concatenate(valid_preds)\n",
        "\n",
        "    return final_loss, valid_preds\n",
        "\n",
        "def inference_fn(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "\n",
        "    for data in dataloader:\n",
        "        inputs = data['x'].to(device)\n",
        "        with torch.no_grad():\n",
        "            outputs = model(inputs)\n",
        "\n",
        "        preds.append(outputs.sigmoid().detach().cpu().numpy())\n",
        "\n",
        "    preds = np.concatenate(preds)\n",
        "\n",
        "    return preds"
      ],
      "metadata": {
        "id": "K9uqsEsAAppc"
      },
      "id": "K9uqsEsAAppc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, num_features, num_targets, hidden_size):\n",
        "        super(Model, self).__init__()\n",
        "        cha_1 = 256\n",
        "        cha_2 = 512\n",
        "        cha_3 = 512\n",
        "\n",
        "        cha_1_reshape = int(hidden_size/cha_1)\n",
        "        cha_po_1 = int(hidden_size/cha_1/2)\n",
        "        cha_po_2 = int(hidden_size/cha_1/2/2) * cha_3\n",
        "\n",
        "        self.cha_1 = cha_1\n",
        "        self.cha_2 = cha_2\n",
        "        self.cha_3 = cha_3\n",
        "        self.cha_1_reshape = cha_1_reshape\n",
        "        self.cha_po_1 = cha_po_1\n",
        "        self.cha_po_2 = cha_po_2\n",
        "\n",
        "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
        "\n",
        "        self.batch_norm_c1 = nn.BatchNorm1d(cha_1)\n",
        "        self.dropout_c1 = nn.Dropout(0.1)\n",
        "        self.conv1 = nn.utils.weight_norm(nn.Conv1d(cha_1,cha_2, kernel_size = 5, stride = 1, padding=2,  bias=False),dim=None)\n",
        "\n",
        "        self.ave_po_c1 = nn.AdaptiveAvgPool1d(output_size = cha_po_1)\n",
        "\n",
        "        self.batch_norm_c2 = nn.BatchNorm1d(cha_2)\n",
        "        self.dropout_c2 = nn.Dropout(0.1)\n",
        "        self.conv2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n",
        "\n",
        "        self.batch_norm_c2_1 = nn.BatchNorm1d(cha_2)\n",
        "        self.dropout_c2_1 = nn.Dropout(0.3)\n",
        "        self.conv2_1 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_2, kernel_size = 3, stride = 1, padding=1, bias=True),dim=None)\n",
        "\n",
        "        self.batch_norm_c2_2 = nn.BatchNorm1d(cha_2)\n",
        "        self.dropout_c2_2 = nn.Dropout(0.2)\n",
        "        self.conv2_2 = nn.utils.weight_norm(nn.Conv1d(cha_2,cha_3, kernel_size = 5, stride = 1, padding=2, bias=True),dim=None)\n",
        "\n",
        "        self.max_po_c2 = nn.MaxPool1d(kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "        self.flt = nn.Flatten()\n",
        "\n",
        "        self.batch_norm3 = nn.BatchNorm1d(cha_po_2)\n",
        "        self.dropout3 = nn.Dropout(0.2)\n",
        "        self.dense3 = nn.utils.weight_norm(nn.Linear(cha_po_2, num_targets))\n",
        "\n",
        "      def forward(self, x):\n",
        "\n",
        "        x = self.batch_norm1(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = F.celu(self.dense1(x), alpha=0.06)\n",
        "\n",
        "        x = x.reshape(x.shape[0],self.cha_1,\n",
        "                      self.cha_1_reshape)\n",
        "\n",
        "        x = self.batch_norm_c1(x)\n",
        "        x = self.dropout_c1(x)\n",
        "        x = F.relu(self.conv1(x))\n",
        "\n",
        "        x = self.ave_po_c1(x)\n",
        "\n",
        "        x = self.batch_norm_c2(x)\n",
        "        x = self.dropout_c2(x)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x_s = x\n",
        "\n",
        "        x = self.batch_norm_c2_1(x)\n",
        "        x = self.dropout_c2_1(x)\n",
        "        x = F.relu(self.conv2_1(x))\n",
        "\n",
        "        x = self.batch_norm_c2_2(x)\n",
        "        x = self.dropout_c2_2(x)\n",
        "        x = F.relu(self.conv2_2(x))\n",
        "        x =  x * x_s\n",
        "\n",
        "        x = self.max_po_c2(x)\n",
        "\n",
        "        x = self.flt(x)\n",
        "\n",
        "        x = self.batch_norm3(x)\n",
        "        x = self.dropout3(x)\n",
        "        x = self.dense3(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "dE_1bcomA0CJ"
      },
      "id": "dE_1bcomA0CJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_training(fold, seed):\n",
        "\n",
        "    seed_everything(seed)\n",
        "\n",
        "    trn_idx = train[train['kfold'] != fold].index\n",
        "    val_idx = train[train['kfold'] == fold].index\n",
        "\n",
        "    train_df = train[train['kfold'] != fold].reset_index(drop=True).copy()\n",
        "    valid_df = train[train['kfold'] == fold].reset_index(drop=True).copy()\n",
        "\n",
        "    x_train, y_train,y_train_ns = train_df[feature_cols], train_df[target_cols].values,train_df[target_nonsc_cols2].values\n",
        "    x_valid, y_valid,y_valid_ns  =  valid_df[feature_cols], valid_df[target_cols].values,valid_df[target_nonsc_cols2].values\n",
        "    x_test = test_[feature_cols]\n",
        "\n",
        "    #------------ norm --------------\n",
        "    col_num = list(set(feat_dic['gene'] + feat_dic['cell']) & set(feature_cols))\n",
        "    col_num.sort()\n",
        "    x_train[col_num],ss = norm_fit(x_train[col_num],True,'quan')\n",
        "    x_valid[col_num]    = norm_tra(x_valid[col_num],ss)\n",
        "    x_test[col_num]     = norm_tra(x_test[col_num],ss)\n",
        "\n",
        "    #------------ pca --------------\n",
        "    def pca_pre(tr,va,te,\n",
        "                n_comp,feat_raw,feat_new):\n",
        "        pca = PCA(n_components=n_comp, random_state=42)\n",
        "        tr2 = pd.DataFrame(pca.fit_transform(tr[feat_raw]),columns=feat_new)\n",
        "        va2 = pd.DataFrame(pca.transform(va[feat_raw]),columns=feat_new)\n",
        "        te2 = pd.DataFrame(pca.transform(te[feat_raw]),columns=feat_new)\n",
        "        return(tr2,va2,te2)\n",
        "\n",
        "\n",
        "    pca_feat_g = [f'pca_G-{i}' for i in range(n_comp1)]\n",
        "    feat_dic['pca_g'] = pca_feat_g\n",
        "    x_tr_g_pca,x_va_g_pca,x_te_g_pca = pca_pre(x_train,x_valid,x_test,\n",
        "                                                n_comp1,feat_dic['gene'],pca_feat_g)\n",
        "    x_train = pd.concat([x_train,x_tr_g_pca],axis = 1)\n",
        "    x_valid = pd.concat([x_valid,x_va_g_pca],axis = 1)\n",
        "    x_test  = pd.concat([x_test,x_te_g_pca],axis = 1)\n",
        "\n",
        "    pca_feat_g = [f'pca_C-{i}' for i in range(n_comp2)]\n",
        "    feat_dic['pca_c'] = pca_feat_g\n",
        "    x_tr_c_pca,x_va_c_pca,x_te_c_pca = pca_pre(x_train,x_valid,x_test,\n",
        "                                                n_comp2,feat_dic['cell'],pca_feat_g)\n",
        "    x_train = pd.concat([x_train,x_tr_c_pca],axis = 1)\n",
        "    x_valid = pd.concat([x_valid,x_va_c_pca],axis = 1)\n",
        "    x_test  = pd.concat([x_test,x_te_c_pca], axis = 1)\n",
        "\n",
        "    x_train,x_valid,x_test = x_train.values,x_valid.values,x_test.values\n",
        "\n",
        "    train_dataset = TrainDataset(x_train, y_train_ns)\n",
        "    valid_dataset = TrainDataset(x_valid, y_valid_ns)\n",
        "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    model = Model(\n",
        "        num_features=num_features,\n",
        "        num_targets=num_targets_0,\n",
        "        hidden_size=hidden_size,\n",
        "    )\n",
        "\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=0.0001)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e5, \n",
        "                                              max_lr=0.0001, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
        "\n",
        "    loss_tr = nn.BCEWithLogitsLoss()   #SmoothBCEwLogits(smoothing = 0.001)\n",
        "    loss_va = nn.BCEWithLogitsLoss()    \n",
        "\n",
        "    early_stopping_steps = EARLY_STOPPING_STEPS\n",
        "    early_step = 0\n",
        "\n",
        "    for epoch in range(1):\n",
        "        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n",
        "        valid_loss, valid_preds = valid_fn(model, loss_va, validloader, DEVICE)\n",
        "        print(f\"FOLD: {fold}, EPOCH: {epoch},train_loss: {train_loss}, valid_loss: {valid_loss}\")\n",
        "\n",
        "    model.dense3 = nn.utils.weight_norm(nn.Linear(model.cha_po_2, num_targets))\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    train_dataset = TrainDataset(x_train, y_train)\n",
        "    valid_dataset = TrainDataset(x_valid, y_valid)\n",
        "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
        "                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
        "\n",
        "    loss_tr = SmoothBCEwLogits(smoothing = 0.001)\n",
        "    loss_va = nn.BCEWithLogitsLoss()    \n",
        "\n",
        "    early_stopping_steps = EARLY_STOPPING_STEPS\n",
        "    early_step = 0\n",
        "\n",
        "    oof = np.zeros((len(train), len(target_cols)))\n",
        "    best_loss = np.inf\n",
        "\n",
        "    mod_name = f\"FOLD_mod11_{seed}_{fold}_.pth\"\n",
        "    \n",
        "    for epoch in range(EPOCHS):\n",
        "\n",
        "        train_loss = train_fn(model, optimizer,scheduler, loss_tr, trainloader, DEVICE)\n",
        "        valid_loss, valid_preds = valid_fn(model, loss_va, validloader, DEVICE)\n",
        "        print(f\"SEED: {seed}, FOLD: {fold}, EPOCH: {epoch},train_loss: {train_loss}, valid_loss: {valid_loss}\")\n",
        "\n",
        "        if valid_loss < best_loss:\n",
        "\n",
        "            best_loss = valid_loss\n",
        "            oof[val_idx] = valid_preds\n",
        "            torch.save(model.state_dict(), mod_name)\n",
        "\n",
        "        elif(EARLY_STOP == True):\n",
        "\n",
        "            early_step += 1\n",
        "            if (early_step >= early_stopping_steps):\n",
        "                break\n",
        "\n",
        "    #--------------------- PREDICTION---------------------\n",
        "    testdataset = TestDataset(x_test)\n",
        "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    model = Model(\n",
        "        num_features=num_features,\n",
        "        num_targets=num_targets,\n",
        "        hidden_size=hidden_size,\n",
        "    )\n",
        "\n",
        "    model.load_state_dict(torch.load(mod_name))\n",
        "    model.to(DEVICE)\n",
        "\n",
        "    predictions = np.zeros((len(test_), len(target_cols)))\n",
        "    predictions = inference_fn(model, testloader, DEVICE)\n",
        "    return oof, predictions\n",
        "\n",
        "def run_k_fold(NFOLDS, seed):\n",
        "    oof = np.zeros((len(train), len(target_cols)))\n",
        "    predictions = np.zeros((len(test), len(target_cols)))\n",
        "\n",
        "    for fold in range(NFOLDS):\n",
        "        oof_, pred_ = run_training(fold, seed)\n",
        "\n",
        "        predictions += pred_ / NFOLDS\n",
        "        oof += oof_\n",
        "\n",
        "    return oof, predictions\n",
        "\n",
        "oof_, predictions_ = run_k_fold(NFOLDS, seed)\n",
        "oof += oof_ / len(SEED)\n",
        "predictions += predictions_ / len(SEED)\n",
        "\n",
        "oof_tmp = dp(oof)\n",
        "oof_tmp = oof_tmp * len(SEED) / (SEED.index(seed)+1)\n",
        "sc_dic[seed] = np.mean([log_loss(train[target_cols].iloc[:,i],oof_tmp[:,i]) for i in range(len(target_cols))])"
      ],
      "metadata": {
        "id": "fLlW08VZA-sn"
      },
      "id": "fLlW08VZA-sn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import log_loss\n",
        "print(np.mean([log_loss(train[target_cols].iloc[:,i],oof[:,i]) for i in range(len(target_cols))]))\n",
        "\n",
        "train0[target_cols] = oof\n",
        "test[target_cols] = predictions\n",
        "\n",
        "### for blend test ###\n",
        "train0.to_csv('train_pred.csv', index=False)\n",
        "### for blend test ###\n",
        "\n",
        "sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n",
        "sub.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "63eQRKzgBSKM"
      },
      "id": "63eQRKzgBSKM",
      "execution_count": null,
      "outputs": []
    }
  ]
}
